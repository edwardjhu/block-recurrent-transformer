seed: 48
# training
window_len: 5
batch_size: 64
warmup: 100
gclip: 2
lr: 0.0003
adam_beta1: 0.9
adam_beta2: 0.98
max_updates: 50000
max_recon_updates: 0
# eval
use_mean: False
# architecture
d_model: 768
d_latent: 128
num_layers: 6
lstm_layers: 4
state_len: 1
bottom_up: False
wandb_tag: lstm
# IO
load_posterior: posterior_model_after_recon.pt
load_decoder: model_after_recon.pt